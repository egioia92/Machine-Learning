# -*- coding: utf-8 -*-
"""Analyzing Customer Reviews for Music Genre Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12EaZKfoOVNV4Bov_yBXnZAGXU7seMi5i

# Analyzing Customer Reviews for Music Genre Classification

This project classifies music album reviews into genres using a combination of:
- Bag-of-Words (BoW)
- Semantic features
- Sentiment analysis.

The dataset includes reviews from **8 music genres** after filtering some categories.

# Import Libraries
This section imports all necessary libraries for data analysis, visualization, natural language processing, and machine learning.
"""

# Install if needed:
!pip install wordcloud
!pip install nrclex

# Core Libraries
import json
import random
import csv
import re

# Data Handling and Analysis
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Natural Language Processing
import nltk
from nltk import word_tokenize, pos_tag
from nltk.tag.mapping import map_tag
from nltk.corpus import stopwords
from string import punctuation
from string import digits
from collections import defaultdict
from nltk.sentiment import SentimentIntensityAnalyzer
from wordcloud import WordCloud

# Machine Learning
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from scipy.sparse import hstack
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform

# IPython and Display
from IPython import get_ipython
from IPython.display import display

from tabulate import tabulate
from nrclex import NRCLex
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.cluster import KMeans

# NLTK Resources
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger_eng')
nltk.download('universal_tagset')
nltk.download('vader_lexicon')

"""# Data Collection and Overview

### Data Sources:
1. **Album Reviews Dataset**:
   - Contains textual reviews and metadata for albums.
   - Each album is labeled with a genre.
2. **Semantic Features Dataset**:
   - Includes additional contextual information like entities and broader categories.

### Challenges:
- **Heterogeneous Data**: Merging datasets with different structures.
- **Imbalanced Classes**: Unequal distribution of genres.

### Summary Statistics:
- **Total Albums**: 800
- **Genres**: ['Classical', 'Country', 'Dance & Electronic', 'Latin Music', 'Metal', 'Pop', 'Rap & Hip-Hop', 'Rock']
- **Average Review Length**: 200–500 characters


"""

# Load music albums from json file into a dictionary that has as keys the Amazon IDs of the product (Album)
products = json.load(open("/content/dataset_classification.json","r"))

# There are 13 different genres
categories = ['Alternative Rock','Classical','Country','Dance & Electronic','Folk','Jazz','Latin Music','Metal','New Age','Pop','R&B','Rap & Hip-Hop','Rock']

products = {key: products[key] for key in sorted(products.keys())}
items = list(products.items())
random.seed(42)
random.shuffle(items)
products = dict(items)

# Keys are the Amazon IDs of the product (Album)
# amazon-id: The Amazon product id.
# You can visualize the album page in amazon adding this id to the following url "www.amazon.com/dp/"
products.keys()

# Example of Metadata of product item B009XIFF5K - Journey Home
products['B009XIFF5K']

products['B009XIFF5K'].keys()

products['B009XIFF5K']['all_pos']

products['B009XIFF5K']['reviews']

products['B009XIFF5K']['reviews'][0].keys()

products['B009XIFF5K']['reviews'][0]['opinions']
# feature, adjective, score

products['B009XIFF5K']['reviews'][0]['rating']

products['B009XIFF5K']['all_text']

# Categories to remove
categories_to_remove = ['Alternative Rock', 'Folk', 'Jazz', 'New Age', 'R&B']

categories = list(set(categories) - set(categories_to_remove))

filtered_products = {
    key: value
    for key, value in products.items()
    if value['genre'] not in categories_to_remove
}

# Update the products dictionary
products = filtered_products

# Creat a Pandas Dataframe
products_df = pd.DataFrame(products)
products_df = products_df.transpose()
products_df.head()

"""# Exploratory Data Analysis (EDA)

### Key Findings:
1. **Review Length Distribution**:
   - Most reviews are between 250–350 words, varying by genre.
2. **Genre Popularity**:
   - Rock and Pop are the most represented genres in the dataset.
3. **Sentiment Trends**:
   - Genres like "Rap & Hip-Hop" exhibit higher sentiment variability compared to "Classical".

### Visualizations:
- Histograms for review lengths.
- Bar plots showing genre distributions.
- Box plots of sentiment scores by genre.

"""

# @title Distribution of Review Lengths

plt.figure(figsize=(10, 6))
plt.hist(products_df['char_length'], bins=30, edgecolor='black')
plt.title('Distribution of Review Lengths')
plt.xlabel('Character Length')
_ = plt.ylabel('Frequency')

# @title Character Length Distribution by Genre

plt.figure(figsize=(12, 6))
plt.boxplot([products_df['char_length'][products_df['genre'] == g] for g in products_df['genre'].unique()], labels=products_df['genre'].unique())
plt.title('Character Length Distribution by Genre')
plt.xlabel('Genre')
plt.ylabel('Character Length')
_ = plt.xticks(rotation=45, ha='right')

# @title num_reviews

products_df.groupby('num_reviews').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# Frequency of genre
products_df['genre'].value_counts().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)
plt.xlabel("Number of Albums")

# Number of review per genre
products_df.groupby('genre')['num_reviews'].mean().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)
plt.xlabel("Average Number of Reviews")

# reviews lenght per genre
products_df.groupby('genre')['char_length'].mean().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# distribution of words per genre
products_df["Words per review"] = products_df["all_text"].apply(lambda x: len(x.split()))

plt.figure(figsize=(12, 6))
plt.boxplot([products_df['Words per review'][products_df['genre'] == g] for g in products_df['genre'].unique()], labels=products_df['genre'].unique())
plt.title('Words per review')
plt.xlabel('Genre')
plt.ylabel('Number of Words')
_ = plt.xticks(rotation=45, ha='right')

"""# Feature Engineering

### Techniques:
1. **Bag-of-Words**:
   - Used TF-IDF to vectorize textual data.
2. **Semantic Features**:
   - Extracted entities, categories, and broader concepts.
3. **Sentiment Analysis**:
   - Computed features like emotional strength, positive-to-negative ratio.

### Observations:
- Semantic features improved contextual understanding.
- Sentiment features provided discriminative power for emotional genres.

# Test and Training Set - Bag Of Words (BOW)

This section prepares the data for machine learning by splitting it into training and testing sets.
It uses the Bag-of-Words (BOW) approach to represent text data as numerical features.
"""

X_bow = products_df['all_text']
y = products_df['genre']

X_bow_train, X_bow_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)

X_bow.shape, X_bow_train.shape, X_bow_test.shape, y.shape, y_train.shape, y_test.shape

# Initialize vectorizer

# With ngram_range=(1,2): More comprehensive feature set, capturing both individual words and word pairs.
# Useful when context or phrase information is crucial.
# Without ngram_range: Simpler feature set with lower computational overhead.
# Works well when unigram features are sufficient for the task.

vectorizer_bow = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english', ngram_range=(1,2), analyzer='word')
#vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')
#vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english', ngram_range=(1,2), analyzer='word') # preprocessor=custom_preprocessor

# Custom preprocessing function
def custom_preprocessor(text):
    # Convert to lowercase
    text = text.lower()
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)  # Keeps only words and spaces
    # Remove numbers
    text = re.sub(r'\d+', '', text)
    return text

# Create TF-IDF features
X_bow_train = vectorizer_bow.fit_transform(X_bow_train)
X_bow_test = vectorizer_bow.transform(X_bow_test)

# No need for a scaler here

"""# Semantic features
This section extracts semantic features such as entities, categories, and broaders, from a separate JSON file and converts them to numerical features using TF-IDF.

"""

semantics = json.load(open("/content/semantic_features.json"))

# Make sure that items in semantics have the same order of items in products
semantics = {key: semantics[key] for key in sorted(semantics.keys())}
items = list(semantics.items())
random.seed(42)
random.shuffle(items)
semantics = dict(items)

semantics.keys()

semantics['B009XIFF5K'].keys()

semantics['B009XIFF5K']['broaders']

semantics['B009XIFF5K']['entities']

semantics['B009XIFF5K']['categories']

semantics_df = pd.DataFrame(semantics)
semantics_df = semantics_df.transpose()
print(semantics_df.head())
#semantics_df.loc['B003DQOW1Q']

# Check if any NaN exists in the dataset
has_nan = semantics_df.isnull().values.any()
print("Contains NaN:", has_nan)

# Merge semantics_df with products_df based on their index (Amazon IDs)
merged_df = pd.merge(semantics_df, products_df[['genre']], left_index=True, right_index=True, how='inner')

# Filter out rows where the genre is in categories_to_remove
filtered_semantics_df = merged_df[~merged_df['genre'].isin(categories_to_remove)]

# Remove the 'genre' column as it's no longer needed
filtered_semantics_df = filtered_semantics_df.drop(columns=['genre'])

semantics_df = filtered_semantics_df

X_sem = semantics_df['entities'] + semantics_df['categories'] + semantics_df['broaders'] # This concatenates the lists or series stored in each column for the same row.
X_sem = X_sem.apply(lambda x: ' '.join(str(item) for item in x)) # converts each element to a string and join the strings with a space using

X_sem_train, X_sem_test, y_sem_train, y_sem_test = train_test_split(X_sem, y, test_size=0.2, random_state=42)


X_sem.shape, X_sem_train.shape, X_sem_test.shape, y.shape, y_sem_train.shape, y_sem_test.shape
X_sem

# Create TF-IDF features
sem_vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')
x_sem_train = sem_vectorizer.fit_transform(X_sem_train)
x_sem_test = sem_vectorizer.transform(X_sem_test)

"""# Sentiment Analysis

This section performs sentiment analysis on the album reviews using VADER and extracts sentiment features like emotional strength, ratio, and positive-to-all ratio.

POS tags (Part-of-Speech tags) and Universal POS tags serve the same fundamental purpose of labeling words with their grammatical roles but differ in their scope, granularity, and use cases.

Examples (Penn Treebank POS Tags for English):
NN: Noun, singular or mass (e.g., "cat")
NNS: Noun, plural (e.g., "cats")
VB: Verb, base form (e.g., "run")
VBD: Verb, past tense (e.g., "ran")
JJ: Adjective (e.g., "happy")
RB: Adverb (e.g., "quickly")


Drawbacks:
Complexity: The large number of tags (e.g., ~36 in Penn Treebank) can make models harder to generalize.
Cross-Language Variability: Different languages have distinct tagsets, making multilingual processing harder.

Universal POS tags simplify part-of-speech tagging by providing a coarse-grained, language-agnostic set of tags. They were introduced to support cross-linguistic comparisons and simplify multilingual NLP tasks.

Examples (Universal POS Tags):
NOUN: Noun
VERB: Verb
ADJ: Adjective
ADV: Adverb
PRON: Pronoun
DET: Determiner
ADP: Adposition (prepositions, postpositions)
Advantages of Universal POS Tags:
Simplicity: Only 17 tags cover all major word classes across languages.
Multilingual Consistency: Ideal for tasks requiring cross-language processing, like machine translation or language modeling.
Good for High-Level Analysis: Suitable for applications where fine-grained distinctions aren't critical (e.g., sentiment analysis, topic modeling).

We can map language-specific POS tags (e.g., Penn Treebank) to universal tags using tools like NLTK's map_tag function:
"""

def get_sentiment_features(products):

    sentiment_features = {}

    for id in products:

        neg_count = 0
        pos_count = 0
        sum = 0
        emotion_words = 0

        for review in products[id]['reviews']:
            for feature,adjective,score in review['opinions']:
                if score is not None:
                    if score < 0:
                        neg_count += 1
                    else:
                        pos_count += 1
                    sum += score
                    emotion_words += 1

        nwords = len(products[id]['all_text'].split())
        eRatio = emotion_words*1.0/nwords
        posToAllRatio = pos_count*1.0/(pos_count+neg_count)
        emotionFeatures = {'eStrength':sum*1.0/emotion_words,'eRatio':eRatio,'posToAllRatio':posToAllRatio}
        sentiment_features[id] = emotionFeatures

    return sentiment_features

def get_sentiment_features_vader(products):

    sentiment_features = {}

    product_sentiment_scores = get_sentiment_scores(products,"all_text_pos")

    for id in products:

        neg_count = 0
        pos_count = 0
        sum = 0
        emotion_words = 0

        for score in product_sentiment_scores[id]:
            if score['compound'] < 0:
                neg_count += 1
            else:
                pos_count += 1
            sum += score['compound']
            emotion_words += 1

        nwords = len(products[id]['all_text'].split())
        eRatio = emotion_words*1.0/nwords
        posToAllRatio = pos_count*1.0/(pos_count+neg_count)
        emotionFeatures = {'eStrength':sum*1.0/emotion_words,'eRatio':eRatio,'posToAllRatio':posToAllRatio}
        sentiment_features[id] = emotionFeatures


    return sentiment_features

def get_sentiment_scores(products, target='all_text'):

    # Initialize VADER Sentiment Analyzer
    sia = SentimentIntensityAnalyzer()

    product_sentiment_scores = {}

    for id in products:
        # Analyze sentiment of filtered words
        if target == 'all_text':
            sentiment_words = get_sentiment_words_from_all_text(products[id]['all_text'])
            scores = [sia.polarity_scores(word) for word in sentiment_words]
        else:
            sentiment_words = get_sentiment_words_from_all_text_pos(products[id]['all_text_pos'])
            scores = [sia.polarity_scores(word) for word in sentiment_words]
        product_sentiment_scores[id] = scores

    return product_sentiment_scores

def get_sentiment_words_from_all_text(text):
    text = text.lower()
    remove_punc = str.maketrans('','', punctuation)
    text = text.translate(remove_punc)
    remove_digits = str.maketrans('', '', digits)
    text = text.translate(remove_digits)

    word_tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))

    filtered_sentence = [word for word in word_tokens if word not in stop_words]

    # Tokenize and POS tag
    tokens = word_tokenize(" ".join(filtered_sentence))
    tags = pos_tag(tokens)

    # Map POS tags to Universal POS tags
    universal_tags = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in tags]
    universal_tags

    sentiment_tags = ['ADJ', 'ADV', 'VERB']
    sentiment_words = [word for word, tag in universal_tags if tag in sentiment_tags]

    return sentiment_words

def get_sentiment_words_from_all_text_pos(text):

    pos_list = text.split()

    # Load English stop words
    stop_words = set(stopwords.words('english'))
    # Initialize a defaultdict to store the vocabulary
    vocabulary = defaultdict(list)

    # Process each word_POS pair
    for item in pos_list:
        # Check if the item contains the delimiter '_'
        if '_' in item:
            # Split the word and POS tag
            word, pos = item.rsplit('_', 1)
            # Check if the word is not a stop word
            if word.lower() not in stop_words:
                # Map the Penn Treebank tag to a universal tag
                universal_tag = map_tag('en-ptb', 'universal', pos)
                # Append the word to the list for the corresponding universal tag
                vocabulary[universal_tag].append(word)
        else:
            # Items without '_', print a warning and skip them
            #print(f"Warning: Skipping item '{item}' as it does not contain '_'.")
            pass

    # Convert defaultdict to a regular dictionary for a cleaner output
    filtered_vocabulary = {tag: words for tag, words in vocabulary.items() if tag in ['ADJ', 'ADV', 'VERB']}

    # Convert filtered_vocabulary to a list of words
    sentiment_words_from_all_text_pos = [word for words in filtered_vocabulary.values() for word in words]

    return sentiment_words_from_all_text_pos

products['B00000296B']['all_pos']

dist = nltk.FreqDist(products['B00000296B']['all_pos'].split())
dist

# @title map Penn TreebankPOS tags to universal tags
from collections import Counter

new_dist = Counter()

for tag, count in dist.items():
			new_dist[map_tag('en-ptb', 'universal', tag)] += count

new_dist

new_dist['NOUN']

print(get_sentiment_words_from_all_text_pos(products['B00000296B']['all_text_pos']))

print(get_sentiment_words_from_all_text(products['B00000296B']['all_text']))

wordcloud = WordCloud(background_color="white").generate(str(get_sentiment_words_from_all_text_pos(products['B00000296B']['all_text_pos'])))

plt.figure(figsize = (8, 8))

plt.imshow(wordcloud); plt.axis("off")

plt.tight_layout(pad = 0); plt.show()

sentiment_features_vader = get_sentiment_features_vader(products)
sentiment_features_vader['B00000296B']

sentiment_features = get_sentiment_features(products)
sentiment_features['B00000296B']

sentiment_features = sentiment_features_vader

# Make sure that items in semantics have the same order of items in products
sentiment_features = {key: sentiment_features[key] for key in sorted(sentiment_features.keys())}
items = list(sentiment_features.items())
random.seed(42)
random.shuffle(items)
sentiment_features = dict(items)

sentiment_df = pd.DataFrame(sentiment_features)
sentiment_df = sentiment_df.transpose()
sentiment_df.head(10)

print(sentiment_df.head())

X_sent = sentiment_df

X_sent_train, X_sent_test, y_sent_train, y_sent_test = train_test_split(X_sent, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
scaler.fit(X_sent_train)

x_sent_train = scaler.transform(X_sent_train)
x_sent_test = scaler.transform(X_sent_test)

print(X_sent_train.shape)

"""# Classification Analysis - BOW
This section trains and evaluates different classifiers using the BOW features to predict the genre of music albums.
"""

# Initialize classifiers
logistic_regression = LogisticRegression(max_iter=1000)
random_forest = RandomForestClassifier(n_estimators=100)
linear_svm = LinearSVC(loss='squared_hinge', penalty="l2", dual=False, tol=1e-3)
neural_network = MLPClassifier(hidden_layer_sizes=(100,50), max_iter=500)


# Train and evaluate each classifier
def evaluate_model(model, x_train, y_train, x_test, y_test):
    model.fit(x_train, y_train)
    predictions = model.predict(x_test)
    accuracy = accuracy_score(y_test, predictions)
    plot_confusion_matrix(y_test, predictions, categories)
    return accuracy

def plot_confusion_matrix(y_true, y_pred, classes):
    cm = confusion_matrix(y_true, y_pred, normalize='true')
    fig, ax = plt.subplots(figsize=(15, 8))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
    disp.plot(cmap='Blues', values_format='.2f', ax=ax)
    plt.title('Confusion Matrix')
    _ = plt.xticks(rotation=45, ha='right')
    plt.show()

# Evaluate Logistic Regression
bow_accuracy_lr = evaluate_model(logistic_regression, X_bow_train, y_train, X_bow_test, y_test)
print(f"Logistic Regression Accuracy: {bow_accuracy_lr:.2f}")

# Evaluate Random Forest
bow_accuracy_rf = evaluate_model(random_forest, X_bow_train, y_train, X_bow_test, y_test)
print(f"Random Forest Accuracy: {bow_accuracy_rf:.2f}")

# Evaluate Linear SVM
bow_accuracy_svm = evaluate_model(linear_svm, X_bow_train, y_train, X_bow_test, y_test)
print(f"Linear SVM Accuracy: {bow_accuracy_svm:.2f}")

# Evaluate NN
bow_accuracy_nn = evaluate_model(neural_network, X_bow_train, y_train, X_bow_test, y_test)
print(f"NN Accuracy: {bow_accuracy_nn:.2f}")

"""# Classification Analysis - BOW, SEM

This section combines BOW and semantic features for classification and evaluates the models.
"""

X_bow_sem_train = hstack((X_bow_train,x_sem_train),format='csr')
X_bow_sem_test  = hstack((X_bow_test,x_sem_test),format='csr')

# Evaluate Logistic Regression
bow_sem_accuracy_lr = evaluate_model(logistic_regression, X_bow_sem_train, y_train, X_bow_sem_test, y_test)
print(f"Logistic Regression Accuracy: {bow_sem_accuracy_lr:.2f}")

# Evaluate Random Forest
bow_sem_accuracy_rf = evaluate_model(random_forest, X_bow_sem_train, y_train, X_bow_sem_test, y_test)
print(f"Random Forest Accuracy: {bow_sem_accuracy_rf:.2f}")

# Evaluate Linear SVM
bow_sem_accuracy_svm = evaluate_model(linear_svm, X_bow_sem_train, y_train, X_bow_sem_test, y_test)
print(f"Linear SVM Accuracy: {bow_sem_accuracy_svm:.2f}")

# Evaluate NN
bow_sem_accuracy_nn = evaluate_model(neural_network, X_bow_sem_train, y_train, X_bow_sem_test, y_test)
print(f"NN Accuracy: {bow_sem_accuracy_nn:.2f}")

"""# Classificatio Analysis - BOW, SENT

This section combines BOW and sentiment features for classification and evaluates the models.

"""

X_bow_sent_train = hstack((X_bow_train,x_sent_train),format='csr')
X_bow_sent_test = hstack((X_bow_test,x_sent_test),format='csr')

# Evaluate Logistic Regression
bow_sent_accuracy_lr = evaluate_model(logistic_regression, X_bow_sent_train, y_train, X_bow_sent_test, y_test)
print(f"Logistic Regression Accuracy: {bow_sent_accuracy_lr:.2f}")

# Evaluate Random Forest
bow_sent_accuracy_rf = evaluate_model(random_forest, X_bow_sent_train, y_train, X_bow_sent_test, y_test)
print(f"Random Forest Accuracy: {bow_sent_accuracy_rf:.2f}")

# Evaluate Linear SVM
bow_sent_accuracy_svm = evaluate_model(linear_svm, X_bow_sent_train, y_train, X_bow_sent_test, y_test)
print(f"Linear SVM Accuracy: {bow_sent_accuracy_svm:.2f}")

# Evaluate NN
bow_sent_accuracy_nn = evaluate_model(neural_network, X_bow_sent_train, y_train, X_bow_sent_test, y_test)
print(f"NN Accuracy: {bow_sent_accuracy_svm:.2f}")

"""# Classification Analysis - BOW, SEM, SENT
This section combines BOW, semantic, and sentiment features for classification and evaluates the models.
"""

X_bow_sem_sent_train = hstack((X_bow_train,x_sent_train, x_sem_train),format='csr')
X_bow_sem_sent_test = hstack((X_bow_test,x_sent_test, x_sem_test),format='csr')

# Evaluate Logistic Regression
bow_sem_sent_accuracy_lr = evaluate_model(logistic_regression, X_bow_sem_sent_train, y_train, X_bow_sem_sent_test, y_test)
print(f"Logistic Regression Accuracy: {bow_sem_sent_accuracy_lr:.2f}")

# Evaluate Random Forest
bow_sem_sent_accuracy_rf = evaluate_model(random_forest, X_bow_sem_sent_train, y_train, X_bow_sem_sent_test, y_test)
print(f"Random Forest Accuracy: {bow_sem_sent_accuracy_rf:.2f}")

# Evaluate Linear SVM
bow_sem_sent_accuracy_svm = evaluate_model(linear_svm, X_bow_sem_sent_train, y_train, X_bow_sem_sent_test, y_test)
print(f"Linear SVM Accuracy: {bow_sem_sent_accuracy_svm:.2f}")

# Evaluate NN
bow_sem_sent_accuracy_nn = evaluate_model(neural_network, X_bow_sem_sent_train, y_train, X_bow_sem_sent_test, y_test)
print(f"NN Accuracy: {bow_sem_sent_accuracy_svm:.2f}")

# Evaluate classifiers and combine results
results_combined = []

# Evaluate classifiers for X_bow
results_combined.append(['Logistic Regression', 'X_bow', f"{bow_accuracy_lr:.2f}"])
results_combined.append(['Random Forest', 'X_bow', f"{bow_accuracy_rf:.2f}"])
results_combined.append(['Linear SVM', 'X_bow', f"{bow_accuracy_svm:.2f}"])
results_combined.append(['NN', 'X_bow', f"{bow_accuracy_nn:.2f}"])

# Evaluate classifiers for X_sem
results_combined.append(['Logistic Regression', 'X_bow_sem', f"{bow_sem_accuracy_lr:.2f}"])
results_combined.append(['Random Forest', 'X_bow_sem', f"{bow_sem_accuracy_rf:.2f}"])
results_combined.append(['Linear SVM', 'X_bow_sem', f"{bow_sem_accuracy_svm:.2f}"])
results_combined.append(['NN', 'X_bow_sem', f"{bow_sem_accuracy_nn:.2f}"])

# Evaluate classifiers for X_bow_sent
results_combined.append(['Logistic Regression', 'X_bow_sent', f"{bow_sent_accuracy_lr:.2f}"])
results_combined.append(['Random Forest', 'X_bow_sent', f"{bow_sent_accuracy_rf:.2f}"])
results_combined.append(['Linear SVM', 'X_bow_sent', f"{bow_sent_accuracy_svm:.2f}"])
results_combined.append(['NN', 'X_bow_sent', f"{bow_sent_accuracy_nn:.2f}"])

# Evaluate classifiers for X_bow_sem_sent
results_combined.append(['Logistic Regression', 'X_bow_sem_sent', f"{bow_sem_sent_accuracy_lr:.2f}"])
results_combined.append(['Random Forest', 'X_bow_sem_sent', f"{bow_sem_sent_accuracy_rf:.2f}"])
results_combined.append(['Linear SVM', 'X_bow_sem_sent', f"{bow_sem_sent_accuracy_svm:.2f}"])
results_combined.append(['NN', 'X_bow_sem_sent', f"{bow_sem_sent_accuracy_nn:.2f}"])

# Create a table using pandas
results_df = pd.DataFrame(results_combined, columns=['Classifier', 'Dataset', 'Accuracy'])
results_pivot = results_df.pivot(index='Classifier', columns='Dataset', values='Accuracy')

# Display the table
print(tabulate(results_pivot, headers='keys', tablefmt='grid'))

fig, ax = plt.subplots(figsize=(8, 4))
ax.axis('tight')
ax.axis('off')
table = ax.table(cellText=results_pivot.values,
                 colLabels=results_pivot.columns,
                 rowLabels=results_pivot.index,
                 cellLoc='center',
                 loc='center')
table.auto_set_font_size(False)
table.set_fontsize(10)
table.auto_set_column_width(col=list(range(len(results_pivot.columns))))
plt.show()

results_df = pd.DataFrame(results_combined, columns=['Classifier', 'Dataset', 'Accuracy'])
results_df['Accuracy'] = pd.to_numeric(results_df['Accuracy'])  # Convert 'Accuracy' column to numeric
results_pivot = results_df.pivot(index='Classifier', columns='Dataset', values='Accuracy')

# Plotting
results_pivot.plot(kind='bar', figsize=(10, 6))
plt.ylabel('Accuracy')
plt.title('Classifier Accuracy by Dataset')
plt.legend(title='Dataset')

# Show plot
plt.show()

results_df = pd.DataFrame(results_combined, columns=['Classifier', 'Dataset', 'Accuracy'])
results_df['Accuracy'] = pd.to_numeric(results_df['Accuracy'])  # Convert 'Accuracy' column to numeric
results_pivot = results_df.pivot(index='Classifier', columns='Dataset', values='Accuracy')

# Filter for Ridge Classifier
svm_classifier_results = results_pivot.loc[['Linear SVM']]

# Plotting horizontally with accuracy values on bars
ax = svm_classifier_results.plot(kind='barh', figsize=(10, 6))
plt.xlabel('Accuracy')
plt.title('SVM Accuracy by Dataset')
plt.legend(title='Dataset')

# Add accuracy values on bars
for p in ax.patches:
    ax.annotate(f'{p.get_width():.2f}', (p.get_width() + 0.01, p.get_y() + p.get_height() / 2),
                ha='left', va='center', fontsize=10)

# Show plot
plt.show()

# Create a list of POS tag sequences for each product
X_pos = products_df['all_pos'].tolist()
print(X_pos)
# Initialize a TF-IDF vectorizer
pos_vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')

# Create TF-IDF features for POS tags
X_pos_train, X_pos_test, _, _ = train_test_split(X_pos, y, test_size=0.2, random_state=42)

X_pos_train = pos_vectorizer.fit_transform(X_pos_train)
X_pos_test = pos_vectorizer.transform(X_pos_test)

# Combine features (BOW, Sentiment, POS)
X_combined_train = hstack((X_bow_train, x_sent_train, X_pos_train), format='csr')
X_combined_test = hstack((X_bow_test, x_sent_test, X_pos_test), format='csr')

# Initialize and train your classifier (e.g., Logistic Regression)
logistic_regression = LogisticRegression(max_iter=1000)
logistic_regression.fit(X_combined_train, y_train)

# Make predictions and evaluate
predictions = logistic_regression.predict(X_combined_test)
accuracy = accuracy_score(y_test, predictions)
print(f"Accuracy with POS features: {accuracy:.2f}")

# Evaluate classifiers for X_bow_sem_sent
results_combined.append(['Logistic Regression', 'X_bow_pos', f"{evaluate_model(logistic_regression, X_combined_train, y_train, X_combined_test, y_test):.2f}"])
results_combined.append(['Random Forest', 'X_bow_pos', f"{evaluate_model(random_forest, X_combined_train, y_train, X_combined_test, y_test):.2f}"])
results_combined.append(['Linear SVM', 'X_bow_pos', f"{evaluate_model(linear_svm, X_combined_train, y_train, X_combined_test, y_test):.2f}"])

# Create a table using pandas
results_df = pd.DataFrame(results_combined, columns=['Classifier', 'Dataset', 'Accuracy'])
results_pivot = results_df.pivot(index='Classifier', columns='Dataset', values='Accuracy')

# Display the table
print(tabulate(results_pivot, headers='keys', tablefmt='grid'))

"""# Modeling and Evaluation

### Models Tested:
1. Logistic Regression
2. Random Forest
3. Linear SVM
4. Neural Networks

### Results:
- **Best Performing Model**: Linear SVM with semantic and sentiment features.
  - **Accuracy**: 72%
- Sentiment features alone slightly improved classification compared to Bag-of-Words.

# Finetuning Hyperparams
"""

lr_exploration  = False
svm_exploration = False
rf_exploration  = False
nn_exploration  = False

if lr_exploration:

    print("Logistic Regression Exploration start")
    print(" ")

    param_grid = {
        'C': [0.1, 1, 10],
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear', 'saga']  # Both support 'l1' and 'l2'
    }

    grid_search = GridSearchCV(logistic_regression, param_grid, cv=5)
    grid_search.fit(X_bow_train, y_train)
    best_model = grid_search.best_estimator_

    print("Best Parameters for Logistic Regression:", grid_search.best_params_)
    print("Best Cross-Validation Score for Logistic Regression:", grid_search.best_score_)
    print("Logistic Regression Exploration Stop")
    print(" ")

    print("Linear SVC Exploration start")
    print(" ")

if svm_exploration:

    # Define LinearSVC with fixed settings
    linear_svm = LinearSVC(dual=False, max_iter=1000)

    # Define the parameter grid for tuning
    svm_param_grid = {
        'C': [0.01, 0.1, 1, 10, 100],               # Regularization strength
        'loss': ['hinge', 'squared_hinge'],         # Loss functions
        'tol': [1e-4, 1e-3, 1e-2],                  # Tolerance for stopping criteria
        'penalty': ['l1', 'l2']                     # Penalty term
    }

    # Perform Grid Search
    svm_grid_search = GridSearchCV(linear_svm, svm_param_grid, cv=5, scoring='accuracy')
    svm_grid_search.fit(X_bow_train, y_train)

    # Display best parameters and score
    print("Linear SVM Best Parameters:", svm_grid_search.best_params_)
    print("Linear SVM Best Score:", svm_grid_search.best_score_)
    print("Linear SVC Exploration stop")
    print(" ")

if rf_exploration:

    print("Random Forest Exploration start")
    print(" ")

    # Define Random Forest and parameter grid
    random_forest = RandomForestClassifier()
    rf_param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [None, 10, 20, 30],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }

    # Perform Grid Search
    rf_grid_search = GridSearchCV(random_forest, rf_param_grid, cv=5, scoring='accuracy')
    rf_grid_search.fit(X_bow_train, y_train)

    # Best parameters and score
    print("Random Forest Best Parameters:", rf_grid_search.best_params_)
    print("Random Forest Best Score:", rf_grid_search.best_score_)

    print("Random Forest Exploration stop")
    print(" ")

if nn_exploration:

    # Reduced parameter grid for RandomizedSearchCV
    param_distributions = {
        'hidden_layer_sizes': [(100, 50), (100, 50,25), (100, 75, 50, 25), (100, 100, 50, 50), (100, 75, 50, 25, 10) ],
        'activation': ['relu', 'tanh'],  # Common choices
        'solver': ['adam'],  # Focused on the best-performing solver
        'alpha': uniform(0.0001, 0.01),  # Sample from a continuous range
        'learning_rate': ['adaptive'],  # Popular option for MLP
        'max_iter': [200, 300]  # Limited iterations to reduce time
    }

    # Initialize MLPClassifier with early stopping
    neural_network = MLPClassifier(early_stopping=True, warm_start=True)

    # RandomizedSearchCV with parallel processing and reduced iterations
    randomized_search = RandomizedSearchCV(
        estimator=neural_network,
        param_distributions=param_distributions,
        n_iter=30,  # Randomly sample 30 combinations
        cv=3,  # Fewer folds to save time
        scoring='accuracy',
        verbose=2,
        n_jobs=-1,  # Utilize all available CPU cores
        random_state=42
    )

    # Train on a subset of the data (optional: comment out if using full data)
    subset_indices = np.random.choice(X_bow_train.shape[0], size=min(5000, X_bow_train.shape[0]), replace=False)
    X_subset, y_subset = X_bow_train[subset_indices], y_train[subset_indices]

    # Perform hyperparameter tuning on the subset
    randomized_search.fit(X_subset, y_subset)

    # Display results
    print("Best Parameters:", randomized_search.best_params_)
    print("Best Score:", randomized_search.best_score_)

    # Evaluate the best model on the full test set
    best_model = randomized_search.best_estimator_
    accuracy = evaluate_model(best_model, X_bow_train, y_train, X_bow_test, y_test)  # Replace with your evaluation function
    print(f"Optimized Neural Network Accuracy: {accuracy:.2f}")

# Initialize classifiers
logistic_regression = LogisticRegression(max_iter=1000)
random_forest = RandomForestClassifier( max_depth = None, min_samples_leaf = 2, min_samples_split = 5, n_estimators = 200)
linear_svm = LinearSVC(loss='squared_hinge', tol=1e-3, C = 10, penalty = 'l2')

neural_network = MLPClassifier(
    activation='tanh',
    alpha=0.0019340450985343381,
    hidden_layer_sizes=(100, 100, 50, 50),
    learning_rate='adaptive',
    max_iter=300,
    solver='adam',
    random_state=42  # Add this for reproducibility
)

# Evaluate classifiers and combine results
results_combined_after_grid = []

# Evaluate classifiers for X_bow
results_combined_after_grid.append(['Logistic Regression', 'X_bow', f"{evaluate_model(logistic_regression, X_bow_train, y_train, X_bow_test, y_test):.2f}"])
results_combined_after_grid.append(['Random Forest', 'X_bow', f"{evaluate_model(random_forest, X_bow_train, y_train, X_bow_test, y_test):.2f}"])
results_combined_after_grid.append(['Linear SVM', 'X_bow', f"{evaluate_model(linear_svm, X_bow_train, y_train, X_bow_test, y_test):.2f}"])
results_combined_after_grid.append(['Neural Network', 'X_bow', f"{evaluate_model(neural_network, X_bow_train, y_train, X_bow_test, y_test):.2f}"])

# Evaluate classifiers for X_sem
results_combined_after_grid.append(['Logistic Regression', 'X_bow_sem', f"{evaluate_model(logistic_regression, X_bow_sem_train, y_train, X_bow_sem_test, y_test):.2f}"])
results_combined_after_grid.append(['Random Forest', 'X_bow_sem', f"{evaluate_model(random_forest, X_bow_sem_train, y_train, X_bow_sem_test, y_test):.2f}"])
results_combined_after_grid.append(['Linear SVM', 'X_bow_sem', f"{evaluate_model(linear_svm, X_bow_sem_train, y_train, X_bow_sem_test, y_test):.2f}"])
results_combined_after_grid.append(['Neural Network', 'X_bow_sem', f"{evaluate_model(neural_network, X_bow_sem_train, y_train, X_bow_sem_test, y_test):.2f}"])

# Evaluate classifiers for X_bow_sent
results_combined_after_grid.append(['Logistic Regression', 'X_bow_sent', f"{evaluate_model(logistic_regression, X_bow_sent_train, y_train, X_bow_sent_test, y_test):.2f}"])
results_combined_after_grid.append(['Random Forest', 'X_bow_sent', f"{evaluate_model(random_forest, X_bow_sent_train, y_train, X_bow_sent_test, y_test):.2f}"])
results_combined_after_grid.append(['Linear SVM', 'X_bow_sent', f"{evaluate_model(linear_svm, X_bow_sent_train, y_train, X_bow_sent_test, y_test):.2f}"])
results_combined_after_grid.append(['Neural Network', 'X_bow_sent', f"{evaluate_model(neural_network, X_bow_sent_train, y_train, X_bow_sent_test, y_test):.2f}"])

# Evaluate classifiers for X_bow_sem_sent
results_combined_after_grid.append(['Logistic Regression', 'X_bow_sem_sent', f"{evaluate_model(logistic_regression, X_bow_sem_sent_train, y_train, X_bow_sem_sent_test, y_test):.2f}"])
results_combined_after_grid.append(['Random Forest', 'X_bow_sem_sent', f"{evaluate_model(random_forest, X_bow_sem_sent_train, y_train, X_bow_sem_sent_test, y_test):.2f}"])
results_combined_after_grid.append(['Linear SVM', 'X_bow_sem_sent', f"{evaluate_model(linear_svm, X_bow_sem_sent_train, y_train, X_bow_sem_sent_test, y_test):.2f}"])
results_combined_after_grid.append(['Neural Network', 'X_bow_sem_sent', f"{evaluate_model(neural_network, X_bow_sem_sent_train, y_train, X_bow_sem_sent_test, y_test):.2f}"])

# Create a table using pandas
results_after_grid_df = pd.DataFrame(results_combined_after_grid, columns=['Classifier', 'Dataset', 'Accuracy'])
results_after_grid_pivot = results_after_grid_df.pivot(index='Classifier', columns='Dataset', values='Accuracy')

# Display the table
print(tabulate(results_after_grid_pivot, headers='keys', tablefmt='grid'))

# Convert 'Accuracy' column to numeric for plotting
results_after_grid_df['Accuracy'] = pd.to_numeric(results_after_grid_df['Accuracy'])

# Plotting the table
fig, ax = plt.subplots(figsize=(8, 4))
ax.axis('tight')
ax.axis('off')
table = ax.table(cellText=results_after_grid_pivot.values,
                 colLabels=results_after_grid_pivot.columns,
                 rowLabels=results_after_grid_pivot.index,
                 cellLoc='center',
                 loc='center')
table.auto_set_font_size(False)
table.set_fontsize(10)
table.auto_set_column_width(col=list(range(len(results_after_grid_pivot.columns))))
plt.show()

# Plotting the bar plot
# Convert Accuracy values to numeric before plotting
results_after_grid_pivot = results_after_grid_pivot.astype(float) #This line converts the 'Accuracy' values to numeric (float) before plotting
results_after_grid_pivot.plot(kind='bar', figsize=(10, 6))
plt.ylabel('Accuracy')
plt.title('Classifier Accuracy by Dataset (After Grid Search)')
plt.legend(title='Dataset')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

"""# Findings

### Key Insights:
1. Combining semantic and sentiment features significantly boosts classification accuracy.
2. Sentiment analysis captures genre-specific emotional tones effectively.
3. Genre overlaps highlight nuanced sentiment and semantic similarities.

# Some tests and misc code
"""

# The clustering is driven solely by the sentiment analysis scores,
# allowing you to group albums based on their emotional tone and sentiment expression.

X = sentiment_df

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply KMeans clustering
kmeans = KMeans(n_clusters=8, random_state=42)  # Adjust the number of clusters as needed
products_df['cluster'] = kmeans.fit_predict(X_scaled)


# Analyze the distribution of genres within each cluster
for cluster in range(kmeans.n_clusters):
    print(f"Cluster {cluster}:")
    print(products_df[products_df['cluster'] == cluster]['genre'].value_counts())
    print("\n")

# Apply PCA for dimensionality reduction
pca = PCA(n_components=2)  # Reduce to 2 dimensions
X_pca = pca.fit_transform(X)

# Create a scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=products_df['cluster'], cmap='viridis')
plt.title('Cluster Visualization with PCA')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Cluster')
plt.show()

# Get the most frequent genre for each cluster
cluster_genres = products_df.groupby('cluster')['genre'].agg(lambda x: x.value_counts().index[0])

# Create a scatter plot with genre labels
plt.figure(figsize=(10, 6))
for cluster, genre in cluster_genres.items():
    plt.scatter(
        X_pca[products_df['cluster'] == cluster, 0],
        X_pca[products_df['cluster'] == cluster, 1],
        label=genre,
    )
plt.title('Cluster Visualization with Genre Labels')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()

# Assuming you have a dataframe 'products_df' with genre and sentiment scores (e.g., valence, arousal)
# Create bins for sentiment scores
valence_bins = pd.cut(sentiment_df['eStrength'], bins=3, labels=['Low', 'Medium', 'High'])
arousal_bins = pd.cut(sentiment_df['eRatio'], bins=3, labels=['Low', 'Medium', 'High'])

# Create a pivot table to count genre occurrences in each sentiment bin combination
heatmap_data = pd.pivot_table(
    products_df,
    values='genre',
    index=arousal_bins,
    columns=valence_bins,
    aggfunc='count',
    fill_value=0,
)

# Plot the heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(heatmap_data, annot=True, cmap='viridis', fmt='d')
plt.title('Genre Overlap based on Sentiment Scores (Heatmap)')
plt.xlabel('Valence')
plt.ylabel('Arousal')
plt.show()

sentiment_genre_df = pd.concat([sentiment_df, products_df['genre']], axis=1)
sentiment_genre_df.head()

for feature in ['eStrength', 'eRatio', 'posToAllRatio']:
       plt.figure(figsize=(12, 6))
       sns.boxplot(x='genre', y=feature, data=sentiment_genre_df)
       plt.title(f'Distribution of {feature} by Genre')
       plt.xticks(rotation=45, ha='right')
       plt.show()

def extract_emotion_features(text):
    emotion_features = {'anger': 0, 'fear': 0, 'anticipation': 0, 'trust': 0,
                        'surprise': 0, 'sadness': 0, 'joy': 0, 'disgust': 0,
                        'positive': 0, 'negative': 0}  # Initialize feature dict

    # Analyze emotions using NRCLex
    emotions = NRCLex(text).raw_emotion_scores
    for emotion, score in emotions.items():
        if emotion in emotion_features:
            emotion_features[emotion] += score

    return emotion_features

# Apply the function to each row
products_df['emotion_features'] = products_df['all_text'].apply(extract_emotion_features)

# Expand dictionary columns into separate DataFrame columns
emotion_features_df = products_df['emotion_features'].apply(pd.Series)

# Assuming emotion_features_df contains the emotion feature columns
scaler = StandardScaler()
scaled_emo_features = scaler.fit_transform(emotion_features_df)

# Fit PCA
pca = PCA()
pca.fit(scaled_emo_features)

# Plot explained variance
plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('PCA Elbow Graph')
plt.show()

# Plot individual explained variance
plt.figure(figsize=(8, 5))
plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')
plt.xlabel('Number of Components')
plt.ylabel('Individual Explained Variance Ratio')
plt.title('PCA Explained Variance per Component')
plt.grid()
plt.show()

# Extract PCA loadings
features = emotion_features_df.columns
loadings = pd.DataFrame(pca.components_, columns=features)
loadings.index = [f"PC{i+1}" for i in range(len(pca.components_))]

# Display the loadings
print("Feature Contributions to Principal Components:")
print(loadings)

plt.figure(figsize=(10, 6))
sns.heatmap(loadings, annot=True, cmap="coolwarm", center=0)
plt.title("Feature Contributions to Principal Components")
plt.show()

optimal_components = 5
pca = PCA(n_components=optimal_components)
reduced_features = pca.fit_transform(scaled_emo_features)
print(scaled_emo_features)

# Split emotion_features_df into train and test sets before combining
emotion_features_train, emotion_features_test, _, _ = train_test_split(
    scaled_emo_features, y, test_size=0.2, random_state=42
)

# Combine features (BOW, Sentiment, Emotion)
X_combined_train_new = hstack((X_bow_train, emotion_features_train), format='csr')
X_combined_test_new = hstack((X_bow_test, emotion_features_test), format='csr')

# Initialize and train LinearSVC with increased max_iter and adjusted C
linear_svm = LinearSVC(loss='squared_hinge', tol=1e-3, C=1, penalty='l2', max_iter=5000)

accuracy = evaluate_model(linear_svm, X_combined_train_new, y_train, X_combined_test_new, y_test)
print(f"Accuracy: {accuracy:.2f}")

# Include 'genre' column when concatenating
sentiment_genre_df = pd.concat([products_df[['all_text', 'genre']], emotion_features_df], axis=1)  # Include 'genre' here
print(sentiment_genre_df.head())

for feature in ['anger', 'fear', 'anticipation', 'trust', 'surprise', 'sadness', 'joy', 'disgust', 'positive', 'negative']:
    plt.figure(figsize=(12, 6))
    sns.boxplot(x='genre', y=feature, data=sentiment_genre_df)
    plt.title(f'Distribution of {feature} by Genre')
    plt.xticks(rotation=45, ha='right')
    plt.show()

# Apply feature selection
selector = SelectKBest(score_func=f_classif, k=1000)  # Select top 1000 features
X_selected_train = selector.fit_transform(X_combined_train_new, y_train)
X_selected_test = selector.transform(hstack((X_bow_test, emotion_features_test), format='csr'))

# Initialize and train LinearSVC with increased max_iter and adjusted C
linear_svm = LinearSVC(loss='squared_hinge', tol=1e-3, C=1, penalty='l2', max_iter=5000)

accuracy = evaluate_model(linear_svm, X_selected_train, y_train, X_selected_test, y_test)
print(f"Accuracy: {accuracy:.2f}")

"""# MISC"""

# Save products dictionary
with open('/content/album_reviews.json', 'w') as f:
  json.dump(products, f, indent=4)

# Save semantics_df
semantics_dict = semantics_df.to_dict(orient='index')
with open('/content/album_semantics.json', 'w') as f:
  json.dump(semantics_dict, f, indent=4)

print("Files saved successfully!")