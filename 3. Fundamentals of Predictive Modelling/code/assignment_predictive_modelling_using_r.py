# -*- coding: utf-8 -*-
"""Assignment_Predictive Modelling using R.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18qR63w4Ni_TsQJoe3HJkE9JS_adeb9Ni

# Assignment Predictive Modelling Using R

BACKGROUND

The data for modeling contains information on Selling price of each house in million Rs. It also contains Carpet area in square feet, Distance from nearest metro station and Number of schools within 2 km distance. The data has 198 rows and 5 columns.

Notes:

Dependent Variable:
- Houses selling price

Indepented variables:
- Carpet area in square feet
- Distance from nearest metro station
- Number of schools within 2 km distance

# Question 1 : Import House Price Data. Check the structure of the data

### Import House Price Data. Check the structure of the data

This step involves loading the dataset from a CSV file and displaying the first few rows to understand the structure of the data.
"""

# Import data
data <- read.csv("/content/House Price Data.csv", header=T)

# Display first few rows of data
head(data)

# Check the structure of the data
str(data)

summary(data)

# Count of missing values (NAs) in each column of the dataset data.
colSums(is.na(data))

# Remove the column named "Houseid" from the dataset data
data$Houseid <- NULL
head(data)

"""### Correlation Matrix using Heatmap

A heatmap of the correlation matrix helps visualize the relationships between variables, with color gradients representing the strength of the correlations.
"""

# Load necessary libraries
install.packages("reshape2");
library(ggplot2)
library(reshape2)

# Compute correlation matrix
corr_matrix <- round( cor(data), 2)
corr_matrix
# Melt the correlation matrix
# The melt function transforms (or "melts") a data frame into a long format.
melted_corr_matrix <- melt(corr_matrix)


# Plot heatmap
ggplot(data = melted_corr_matrix, aes(x=Var1, y=Var2, fill=value)) +
geom_tile()+
geom_text(aes(Var2, Var1, label = value), color = "black", size = 4)+
scale_fill_gradient2(low="red",mid="white",high="blue")

"""Observations:
- Price and Area: There is a strong positive correlation. As the area of the house increass, the price of the house tends to increase as well.
- Price and Distance: Moderate negative correlation. As the distance from the metro station increases, the price of the house tends to decrease.
- Price and Schools: There is a positive correlation. Houses close more schools tend to have higher prices.

# Question 2 : Split the data into Training (80%) and Testing (20%) data sets

### Split the data into Training (80%) and Testing (20%) data sets

Splitting the data into training and testing sets allows us to build and evaluate the model on different subsets of the data, which helps in assessing the model's performance.
"""

# Load necessary libraries
install.packages("caret");
library(caret)

# Split data
set.seed(123) # for reproducibility
# 80% of the data is selected for the training set
index <- createDataPartition(data$Price, p=0.8, list=FALSE)

traindata<-data[index,]
testdata<-data[-index,]

"""# Question 3 : Build a regression model on training data to estimate selling price of a House.

### Build a regression model on training data to estimate selling price of a House.

This step involves fitting a linear regression model to the training data to establish the relationship between the dependent variable and the independent variables
"""

# Build model
model<-lm(Price~ ., data = traindata)
model

"""#Question 4 : List down significant variables and interpret their regression coefficients

### List down significant variables and interpret their regression coefficients

The summary of the linear regression model includes parameter estimates (coefficients), R-squared value, p-values, and other statistics that help evaluate the model
"""

# Print the model summary
summary(model)

"""Observations:
- All variables are significant as p values are smaller than 0.05
-  The coefficients for the predictor variables suggest their respective impact on the price. The variables Area and Schools have positive effects, while Distance has a negative effect.

# Question 5 : What is the R2 and adjusted R2 of the model? Give interpretation

### What is the R2 and adjusted R2 of the model? Give interpretation


- The model’s R² value is 0.78, indicating that approximately 78% of the variability in Price is explained by the model.

R² values can sometimes be misleading as adding more predictors to a model can artificially inflate the R² value, even if the predictors are irrelevant.

- The Adjusted R-squared is 0.7798. Adjusted R-squared adjusts the R² statistic based on the number of predictors in the model.
As we can see they are comparable.

Unlike R², the Adjusted R² can decrease if predictors do not improve the model beyond what would be expected by chance.

It is always lower or equal to R², and it provides a more honest representation of the model fit when dealing with multiple predictors.

As we can see they are comparable.

# Question 6 : Is there a multicollinearity problem? If yes, do the necessary steps to remove it

The Variance Inflation Factor (VIF) measures the extent of multicollinearity in the regression model. High VIF values indicate multicollinearity issues.
"""

# Calculate VIF
install.packages("car")
library(car)
vif(model)

"""Observations:
- There is no multicollinearity problem as all VIF’s are less than 5

# Question 7 : Are there any influential observations in the data?

Influential observations in a dataset is an important step in regression analysis, as these points can disproportionately affect the results of the model.
"""

influ<-influence.measures(model)
influ

influencePlot(model,
              id.method="identify",
              main="Influence Plot",
              sub="Circle size is proportioal to Cook's Distance")

"""Observations:
- Observations 17, 32, 35, 98, 116 are influential, as indicated by the large circle sizes. They have a substantial effect on the regression model.

# Question 8 : Can we assume that errors follow ‘Normal’ distribution?

The histogram and Q-Q plot help check if the residuals follow a normal distribution, which is an assumption of linear regression.

If the residuals follow a normal distribution, it validates one of the regression assumptions, supporting the reliability of confidence intervals and hypothesis tests.

If the data are truly sampled from a normal distribution the Q-Q plot will be linear
"""

traindata$fit<-fitted(model)
traindata$res<-residuals(model)

qqnorm(traindata$res)

"""Observation: The points do not seems to sit on a line"""

shapiro.test(traindata$res)

"""The Shapiro-Wilk test, commonly referred to as the Shapiro test, is a statistical test used to assess the normality of a dataset.

the p-value is less than 0.05, the null hypothesis is rejected, indicating that the data do not follow a normal distribution.

The Lilliefors test is a variation of the Kolmogorov-Smirnov (K-S) test that is specifically adjusted for testing the null hypothesis that a dataset comes from a normally distributed population.
"""

install.packages("nortest");
library(nortest)
lillie.test(traindata$res)

"""Observation:  p-value ≤ 0.05 indicates that the data do not follow a normal distribution.

# Question 9 : Is there a Heteroscedasticity problem? Check using residual vs. predictor plots.

We want to diagnose the fit of the model by visualizing the distribution of residuals (errors).

Ideally, residuals should be randomly scattered around zero.

If the residuals are randomly scattered around zero, it suggests that the model’s assumptions are likely satisfied and the model provides a good fit.
"""

plot(traindata$fit,traindata$res)

"""Observation:
- Residuals in the model are randomly distributed indicating NO presence of Heteroscedasticity

# Question 10: Calculate the RMSE for the Training and Testing data. Multiple Linear Regression

The Root Mean Squared Error (RMSE) measures the average magnitude of the errors between predicted and actual values, providing an indication of model accuracy.
"""

RMSE<-sqrt(mean(traindata$res**2))
RMSE

"""Validating the model on the test set using RMSE helps assess its performance on unseen data."""

testdata$pred<-predict(model,testdata)
testdata$res<-(testdata$Price-testdata$pred)
RMSEtest<-sqrt(mean(testdata$res**2))
RMSEtest

"""Obseravation:The similarity in RMSE between the test and train data suggests that the regression model generalizes reasonably well, with consistent prediction accuracy on both datasets. This implies that the model is stable and not overfitting to the training data."""

kfolds<-trainControl(method="cv",number=4)
kmodel<- train(Price~.,data=data,method="lm",
trControl=kfolds)
kmodel

"""Comment: RMSE and R squared values using K-fold validation are similar to overall RMSE and R squared
values
"""